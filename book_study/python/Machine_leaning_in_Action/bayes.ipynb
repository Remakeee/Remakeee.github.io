{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T01:25:55.301003Z",
     "start_time": "2023-10-31T01:25:55.208320Z"
    }
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problem', 'help', 'please'], \\\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], \\\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], \\\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'], \\\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], \\\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    # 1 is abusive, 0 not\n",
    "    classVec = [0, 1, 0, 1, 0, 1]\n",
    "    return postingList, classVec\n",
    "\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    # create empty set\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        # | is union\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)\n",
    "\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    # create vector of all 0s\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            # if word in vocabList, set to 1\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T01:25:55.368550Z",
     "start_time": "2023-10-31T01:25:55.211984Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T01:25:55.371720Z",
     "start_time": "2023-10-31T01:25:55.369526Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainNBO(trainMatrix, trainCategory):\n",
    "    # number of training docs\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # number of words in each doc\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # sum of all abusive docs\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    # create vector of all 0s\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "    # denominator\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # if abusive\n",
    "        if trainCategory[i] == 1:\n",
    "            # add vector to p1Num\n",
    "            p1Num += trainMatrix[i]\n",
    "            # add number of words in doc to p1Denom\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            # add vector to p0Num\n",
    "            p0Num += trainMatrix[i]\n",
    "            # add number of words in doc to p0Denom\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # element-wise division\n",
    "    p1Vect = np.log(p1Num / p1Denom)\n",
    "    p0Vect = np.log(p0Num / p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T01:25:55.377280Z",
     "start_time": "2023-10-31T01:25:55.371834Z"
    }
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    # element-wise multiplication\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1 - pClass1)\n",
    "    # if p1 > p0\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def testingNB():\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    # create vocabulary list\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    # create empty list\n",
    "    trainMat = []\n",
    "    # for each post in list of posts\n",
    "    for postinDoc in listOPosts:\n",
    "        # convert post to vector\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    # train classifier\n",
    "    p0V, p1V, pAb = trainNBO(np.array(trainMat), np.array(listClasses))\n",
    "    # test classifier\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    # convert test entry to vector\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    # classify test entry\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    # convert test entry to vector\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    # classify test entry\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T01:25:55.381605Z",
     "start_time": "2023-10-31T01:25:55.379231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T01:25:55.383357Z",
     "start_time": "2023-10-31T01:25:55.381934Z"
    }
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    # create vector of all 0s\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            # if word in vocabList, increment\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T01:25:55.389621Z",
     "start_time": "2023-10-31T01:25:55.384910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['This',\n 'book',\n 'is',\n 'the',\n 'best',\n 'book',\n 'on',\n 'Python',\n 'or',\n 'M.L.',\n 'I',\n 'have',\n 'ever',\n 'laid',\n 'eyes',\n 'upon.']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    "mySent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T01:25:55.428508Z",
     "start_time": "2023-10-31T01:25:55.388015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['This',\n 'book',\n 'is',\n 'the',\n 'best',\n 'book',\n 'on',\n 'Python',\n 'or',\n 'M.L.',\n 'I',\n 'have',\n 'ever',\n 'laid',\n 'eyes',\n 'upon.']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok for tok in mySent.split() if len(tok) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T01:25:55.434599Z",
     "start_time": "2023-10-31T01:25:55.390736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['this',\n 'book',\n 'is',\n 'the',\n 'best',\n 'book',\n 'on',\n 'python',\n 'or',\n 'm.l.',\n 'i',\n 'have',\n 'ever',\n 'laid',\n 'eyes',\n 'upon.']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lower() for token in mySent.split() if len(token) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T01:25:55.437753Z",
     "start_time": "2023-10-31T01:25:55.393706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['hello,',\n 'since',\n 'you',\n 'are',\n 'an',\n 'owner',\n 'of',\n 'at',\n 'least',\n 'one',\n 'google',\n 'groups',\n 'group',\n 'that',\n 'uses',\n 'the',\n 'customized',\n 'welcome',\n 'message,',\n 'pages',\n 'or',\n 'files,',\n 'we',\n 'are',\n 'writing',\n 'to',\n 'inform',\n 'you',\n 'that',\n 'we',\n 'will',\n 'no',\n 'longer',\n 'be',\n 'supporting',\n 'these',\n 'features',\n 'starting',\n 'february',\n '2011.',\n 'we',\n 'made',\n 'this',\n 'decision',\n 'so',\n 'that',\n 'we',\n 'can',\n 'focus',\n 'on',\n 'improving',\n 'the',\n 'core',\n 'functionalities',\n 'of',\n 'google',\n 'groups',\n '--',\n 'mailing',\n 'lists',\n 'and',\n 'forum',\n 'discussions.',\n 'instead',\n 'of',\n 'these',\n 'features,',\n 'we',\n 'encourage',\n 'you',\n 'to',\n 'use',\n 'products',\n 'that',\n 'are',\n 'designed',\n 'specifically',\n 'for',\n 'file',\n 'storage',\n 'and',\n 'page',\n 'creation,',\n 'such',\n 'as',\n 'google',\n 'docs',\n 'and',\n 'google',\n 'sites.']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailText = open('machinelearninginaction/Ch04/email/ham/6.txt', encoding='gbk').read()\n",
    "listOfTokens = [token.lower() for token in emailText.split() if len(token) > 0]\n",
    "listOfTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello,', 'since', 'you', 'are', 'an', 'owner', 'of', 'at', 'least', 'one', 'google', 'groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message,', 'pages', 'or', 'files,', 'we', 'are', 'writing', 'to', 'inform', 'you', 'that', 'we', 'will', 'no', 'longer', 'be', 'supporting', 'these', 'features', 'starting', 'february', '2011.', 'we', 'made', 'this', 'decision', 'so', 'that', 'we', 'can', 'focus', 'on', 'improving', 'the', 'core', 'functionalities', 'of', 'google', 'groups', '--', 'mailing', 'lists', 'and', 'forum', 'discussions.', 'instead', 'of', 'these', 'features,', 'we', 'encourage', 'you', 'to', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation,', 'such', 'as', 'google', 'docs', 'and', 'google', 'sites.']\n",
      "['hello,', 'since', 'you', 'are', 'an', 'owner', 'of', 'at', 'least', 'one', 'google', 'groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message,', 'pages', 'or', 'files,', 'we', 'are', 'writing', 'to', 'inform', 'you', 'that', 'we', 'will', 'no', 'longer', 'be', 'supporting', 'these', 'features', 'starting', 'february', '2011.', 'we', 'made', 'this', 'decision', 'so', 'that', 'we', 'can', 'focus', 'on', 'improving', 'the', 'core', 'functionalities', 'of', 'google', 'groups', '--', 'mailing', 'lists', 'and', 'forum', 'discussions.', 'instead', 'of', 'these', 'features,', 'we', 'encourage', 'you', 'to', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation,', 'such', 'as', 'google', 'docs', 'and', 'google', 'sites.']\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1, 0]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def textParse(bigString):  # 去掉少于两个字符的字符串，并将所有字符串转换为小写\n",
    "    import re\n",
    "    #print(bigString)\n",
    "    #listOfTokens = re.split(r'\\W*', bigString)\n",
    "    print(listOfTokens)\n",
    "    #print([token.lower() for token in listOfTokens if len(token) > 2])\n",
    "    return [token.lower() for token in bigString.split() if len(token) > 2]\n",
    "docList = []\n",
    "classList = []\n",
    "fullText = []\n",
    "wordList = textParse(open('machinelearninginaction/Ch04/email/spam/1.txt') .read())\n",
    "docList.append(wordList)\n",
    "fullText.extend(wordList)\n",
    "classList.append(1)\n",
    "wordList = textParse(open('machinelearninginaction/Ch04/email/ham/1.txt' ).read())\n",
    "docList.append(wordList)\n",
    "fullText.extend(wordList)\n",
    "classList.append(0)\n",
    "classList"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:25:55.438002Z",
     "start_time": "2023-10-31T01:25:55.400299Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello,', 'since', 'you', 'are', 'an', 'owner', 'of', 'at', 'least', 'one', 'google', 'groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message,', 'pages', 'or', 'files,', 'we', 'are', 'writing', 'to', 'inform', 'you', 'that', 'we', 'will', 'no', 'longer', 'be', 'supporting', 'these', 'features', 'starting', 'february', '2011.', 'we', 'made', 'this', 'decision', 'so', 'that', 'we', 'can', 'focus', 'on', 'improving', 'the', 'core', 'functionalities', 'of', 'google', 'groups', '--', 'mailing', 'lists', 'and', 'forum', 'discussions.', 'instead', 'of', 'these', 'features,', 'we', 'encourage', 'you', 'to', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation,', 'such', 'as', 'google', 'docs', 'and', 'google', 'sites.']\n",
      "['hello,', 'since', 'you', 'are', 'an', 'owner', 'of', 'at', 'least', 'one', 'google', 'groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message,', 'pages', 'or', 'files,', 'we', 'are', 'writing', 'to', 'inform', 'you', 'that', 'we', 'will', 'no', 'longer', 'be', 'supporting', 'these', 'features', 'starting', 'february', '2011.', 'we', 'made', 'this', 'decision', 'so', 'that', 'we', 'can', 'focus', 'on', 'improving', 'the', 'core', 'functionalities', 'of', 'google', 'groups', '--', 'mailing', 'lists', 'and', 'forum', 'discussions.', 'instead', 'of', 'these', 'features,', 'we', 'encourage', 'you', 'to', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation,', 'such', 'as', 'google', 'docs', 'and', 'google', 'sites.']\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordList = textParse(open('machinelearninginaction/Ch04/email/spam/2.txt') .read())\n",
    "docList.append(wordList)\n",
    "fullText.extend(wordList)\n",
    "classList.append(1)\n",
    "wordList = textParse(open('machinelearninginaction/Ch04/email/ham/2.txt' ).read())\n",
    "docList.append(wordList)\n",
    "fullText.extend(wordList)\n",
    "classList.append(0)\n",
    "classList[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:25:55.438110Z",
     "start_time": "2023-10-31T01:25:55.405283Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T01:25:55.438240Z",
     "start_time": "2023-10-31T01:25:55.409740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ndef spamTest():\\n    docList = []\\n    classList = []\\n    fullText = []\\n    for i in range(17,18):\\n        print(i)\\n        wordList = textParse(open(\\'machinelearninginaction/Ch04/email/spam/%d.txt\\' % i).read())\\n        #print(wordList)\\n        # add to doc list\\n        docList.append(wordList)\\n        # add to full text\\n        fullText.extend(wordList)\\n        # add to class list\\n        classList.append(1)\\n        # read email\\n        wordList = textParse(open(\\'machinelearninginaction/Ch04/email/ham/%d.txt\\' % i).read())\\n        # add to doc list\\n        docList.append(wordList)\\n        # add to full text\\n        fullText.extend(wordList)\\n        # add to class list\\n        classList.append(0)\\n    # create vocabulary list\\n    vocabList = createVocabList(docList)\\n    # create empty list\\n    trainingSet = list(range(50))\\n    # create empty list\\n    testSet = []\\n    # for 10 times\\n    for i in range(10):\\n        # choose random index\\n        randIndex = int(np.random.uniform(0, len(trainingSet)))\\n        # add to test set\\n        testSet.append(trainingSet[randIndex])\\n        # delete from training set\\n        del (trainingSet[randIndex])\\n    # create empty list\\n    trainMat = []\\n    # create empty list\\n    trainClasses = []\\n    # for each index in training set\\n    for docIndex in trainingSet:\\n        # convert doc to vector\\n        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\\n        # add to train classes\\n        trainClasses.append(classList[docIndex])\\n    # train classifier\\n    p0V, p1V, pSpam = trainNBO(np.array(trainMat), np.array(trainClasses))\\n    # create empty list\\n    errorCount = 0\\n    # for each index in test set\\n    for docIndex in testSet:\\n        # convert doc to vector\\n        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\\n        # classify doc\\n        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\\n            # increment error count\\n            errorCount += 1\\n            # print email text\\n    print(\"classification error\", docList[docIndex])\\n    print(docList)\\n'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def textParse(bigString):  # 去掉少于两个字符的字符串，并将所有字符串转换为小写\n",
    "    import re\n",
    "    #print(bigString)\n",
    "    #listOfTokens = re.split(r'\\W*', bigString)\n",
    "    print(listOfTokens)\n",
    "    #print([token.lower() for token in listOfTokens if len(token) > 2])\n",
    "    return [token.lower() for token in bigString.split() if len(token) > 2]'''    \n",
    "'''\n",
    "def spamTest():\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    for i in range(17,18):\n",
    "        print(i)\n",
    "        wordList = textParse(open('machinelearninginaction/Ch04/email/spam/%d.txt' % i).read())\n",
    "        #print(wordList)\n",
    "        # add to doc list\n",
    "        docList.append(wordList)\n",
    "        # add to full text\n",
    "        fullText.extend(wordList)\n",
    "        # add to class list\n",
    "        classList.append(1)\n",
    "        # read email\n",
    "        wordList = textParse(open('machinelearninginaction/Ch04/email/ham/%d.txt' % i).read())\n",
    "        # add to doc list\n",
    "        docList.append(wordList)\n",
    "        # add to full text\n",
    "        fullText.extend(wordList)\n",
    "        # add to class list\n",
    "        classList.append(0)\n",
    "    # create vocabulary list\n",
    "    vocabList = createVocabList(docList)\n",
    "    # create empty list\n",
    "    trainingSet = list(range(50))\n",
    "    # create empty list\n",
    "    testSet = []\n",
    "    # for 10 times\n",
    "    for i in range(10):\n",
    "        # choose random index\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        # add to test set\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        # delete from training set\n",
    "        del (trainingSet[randIndex])\n",
    "    # create empty list\n",
    "    trainMat = []\n",
    "    # create empty list\n",
    "    trainClasses = []\n",
    "    # for each index in training set\n",
    "    for docIndex in trainingSet:\n",
    "        # convert doc to vector\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        # add to train classes\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    # train classifier\n",
    "    p0V, p1V, pSpam = trainNBO(np.array(trainMat), np.array(trainClasses))\n",
    "    # create empty list\n",
    "    errorCount = 0\n",
    "    # for each index in test set\n",
    "    for docIndex in testSet:\n",
    "        # convert doc to vector\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        # classify doc\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            # increment error count\n",
    "            errorCount += 1\n",
    "            # print email text\n",
    "    print(\"classification error\", docList[docIndex])\n",
    "    print(docList)\n",
    "'''    \n",
    "# spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "60"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "ny = feedparser.parse('http://www.nasa.gov/rss/dyn/image_of_the_day.rss')\n",
    "len(ny['entries'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:25:56.133981Z",
     "start_time": "2023-10-31T01:25:55.414219Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def textParse(bigString):  # 去掉少于两个字符的字符串，并将所有字符串转换为小写\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [token.lower() for token in listOfTokens if len(token) > 2]\n",
    "\n",
    "    \n",
    "\n",
    "def calcMostFreq(vocabList, fullText):\n",
    "    # create empty dictionary\n",
    "    freqDict = {}\n",
    "    # for each token in vocab list\n",
    "    for token in vocabList:\n",
    "        # add to dictionary\n",
    "        freqDict[token] = fullText.count(token)\n",
    "    # create empty list\n",
    "    sortedFreq = sorted(freqDict.items(), key=lambda x: x[1], reverse=True)\n",
    "    # return top 30 words\n",
    "    return sortedFreq[:30]\n",
    "\n",
    "def localWords(feed1, feed0):\n",
    "    import feedparser\n",
    "    # create empty list\n",
    "    docList = []\n",
    "    # create empty list\n",
    "    classList = []\n",
    "    # create empty list\n",
    "    fullText = []\n",
    "    # minimum length\n",
    "    minLen = min(len(feed1['entries']), len(feed0['entries']))\n",
    "    # for each entry in feed 1\n",
    "    for i in range(minLen):\n",
    "        # get entry\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        # add to doc list\n",
    "        docList.append(wordList)\n",
    "        # add to full text\n",
    "        fullText.extend(wordList)\n",
    "        # add to class list\n",
    "        classList.append(1)\n",
    "        # get entry\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        # add to doc list\n",
    "        docList.append(wordList)\n",
    "        # add to full text\n",
    "        fullText.extend(wordList)\n",
    "        # add to class list\n",
    "        classList.append(0)\n",
    "    # create vocabulary list\n",
    "    vocabList = createVocabList(docList)\n",
    "    # get top 30 words\n",
    "    top30Words = calcMostFreq(vocabList, fullText)\n",
    "    # for each word in top 30 words\n",
    "    for pairW in top30Words:\n",
    "        # if word in vocab list\n",
    "        if pairW[0] in vocabList:\n",
    "            # remove from vocab list\n",
    "            vocabList.remove(pairW[0])\n",
    "    # create empty list\n",
    "    trainingSet = list(range(2 * minLen))\n",
    "    # create empty list\n",
    "    testSet = []\n",
    "    # for 20 times\n",
    "    for i in range(20):\n",
    "        # choose random index\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        # add to test set\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        # delete from training set\n",
    "        del (trainingSet[randIndex])\n",
    "    # create empty list\n",
    "    trainMat = []\n",
    "    # create empty list\n",
    "    trainClasses = []\n",
    "    # for each index in training set\n",
    "    for docIndex in trainingSet:\n",
    "        # convert doc to vector\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        # add to train classes\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    # train classifier\n",
    "    p0V, p1V, pSpam = trainNBO(np.array(trainMat), np.array(trainClasses))\n",
    "    # create empty list\n",
    "    errorCount = 0\n",
    "    # for each index in test set\n",
    "    for docIndex in testSet:\n",
    "        # convert doc to vector\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        # classify doc\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            # increment error count\n",
    "            errorCount += 1\n",
    "    # print error rate\n",
    "    print('the error rate is: ', float(errorCount) / len(testSet))\n",
    "    # return vocab list, p0v, and p1v\n",
    "    return vocabList, p0V, p1V\n",
    "            \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:31:07.376480Z",
     "start_time": "2023-10-31T01:31:07.374857Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T01:31:13.509093Z",
     "start_time": "2023-10-31T01:31:11.874824Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m ny \u001B[38;5;241m=\u001B[39m feedparser\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttp://www.nasa.gov/rss/dyn/image_of_the_day.rss\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m sf \u001B[38;5;241m=\u001B[39m feedparser\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttp://sports.yahoo.com/nba/teams/hou/rss.xml\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m vocabList, pSF, pNY \u001B[38;5;241m=\u001B[39m localWords(ny, sf)\n",
      "Cell \u001B[0;32mIn[19], line 67\u001B[0m, in \u001B[0;36mlocalWords\u001B[0;34m(feed1, feed0)\u001B[0m\n\u001B[1;32m     65\u001B[0m randIndex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39muniform(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(trainingSet)))\n\u001B[1;32m     66\u001B[0m \u001B[38;5;66;03m# add to test set\u001B[39;00m\n\u001B[0;32m---> 67\u001B[0m testSet\u001B[38;5;241m.\u001B[39mappend(trainingSet[randIndex])\n\u001B[1;32m     68\u001B[0m \u001B[38;5;66;03m# delete from training set\u001B[39;00m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m (trainingSet[randIndex])\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ny = feedparser.parse('http://www.nasa.gov/rss/dyn/image_of_the_day.rss')\n",
    "sf = feedparser.parse('http://sports.yahoo.com/nba/teams/hou/rss.xml')\n",
    "vocabList, pSF, pNY = localWords(ny, sf)\n",
    "#vocabList, pSF, pNY = localWords(ny, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-31T01:25:58.060039Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
